{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoukB/tutorial_from_tracking_to_posture_dynamics_temp/blob/main/Part_2_State_space_reconstruction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a83d269",
      "metadata": {
        "id": "4a83d269"
      },
      "source": [
        "# Maximally Predictive Posture Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and installation\n",
        "This first section makes sure you have all the necessary functions to run this tutorial smoothly."
      ],
      "metadata": {
        "id": "gw0iGNlxa3RP"
      },
      "id": "gw0iGNlxa3RP"
    },
    {
      "cell_type": "code",
      "source": [
        "#Install necessary environment for display of videos\n",
        "!pip install -U kora\n",
        "from kora.drive import upload_public\n",
        "from IPython.display import HTML\n"
      ],
      "metadata": {
        "id": "i50Li8ULbDMJ"
      },
      "id": "i50Li8ULbDMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install necessary packages specific to this notebook\n",
        "!pip install umap-learn\n",
        "!pip install umap-learn[plot]\n",
        "!pip install msmtools"
      ],
      "metadata": {
        "id": "E9C2ZvCYboaE"
      },
      "id": "E9C2ZvCYboaE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clone the repository with all files, images and videos. This will result in a folder called cloned-repo\n",
        "!git clone -l -s https://github.com/anoukB/tutorial_from_tracking_to_posture_dynamics cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls  #Listo of all elements in the repo"
      ],
      "metadata": {
        "id": "jSd6NSBddQFy"
      },
      "id": "jSd6NSBddQFy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "LYLBZz9iiiBi"
      },
      "id": "LYLBZz9iiiBi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** STOP HERE :)***\n",
        "\n",
        "You need to set the directory for all the files that will be used in the tutorial. If they have been cloned from GitHub into '/content/cloned-repo/', copy-paste the example. Otherwise, write down the appropriate directory.\n"
      ],
      "metadata": {
        "id": "0wRKr_0AdVX1"
      },
      "id": "0wRKr_0AdVX1"
    },
    {
      "cell_type": "code",
      "source": [
        "#Example:\n",
        "#directory = '/content/cloned-repo/'\n",
        "\n",
        "directory ="
      ],
      "metadata": {
        "id": "b3iKrbPFdV6m"
      },
      "id": "b3iKrbPFdV6m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports for this tutorial\n",
        "import h5py\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.colors as colors\n",
        "import os\n",
        "import scipy\n",
        "from scipy.integrate import odeint\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import umap.plot\n",
        "from mpl_toolkits import mplot3d\n",
        "import math\n",
        "from statsmodels.graphics.tsaplots import plot_acf  #autocorrelation\n",
        "\n",
        "#Load functions from other files\n",
        "import sys\n",
        "path_to_module = directory\n",
        "sys.path.append(path_to_module)\n",
        "import clustering_methods as cl\n",
        "import operator_calculations as op_calc\n",
        "import delay_embedding_1D as embed\n",
        "import matplotlib.animation as anim"
      ],
      "metadata": {
        "id": "1EA06h2qbkEo"
      },
      "id": "1EA06h2qbkEo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a function to display pictures easily\n",
        "def display_picture(url):\n",
        "  im = plt.imread(url)\n",
        "\n",
        "  fig, axs = plt.subplots(ncols=1, nrows=1)\n",
        "  axs.imshow(im, cmap='gray')\n",
        "  axs.set_axis_off()"
      ],
      "metadata": {
        "id": "Z9skfPD6bGlw"
      },
      "id": "Z9skfPD6bGlw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file with the Principal Components TS made in previous tutorial\n",
        "dir_file_storage = directory  # Choose the directory where you put the file\n",
        "filename = 'file_principal_components_time_series_larva.csv'  #Write down your filename\n",
        "PC_ts = np.loadtxt(dir_file_storage + filename  , delimiter = \",\")"
      ],
      "metadata": {
        "id": "S9vIVLL0bUsE"
      },
      "id": "S9vIVLL0bUsE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "272c976d",
      "metadata": {
        "id": "272c976d"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4e29d5",
      "metadata": {
        "id": "0f4e29d5"
      },
      "source": [
        "For the second notebook of this series, we will move from posture space to posture dynamics. How can we map recordings of the animal to interpretable numbers that retain useful information about the dynamics?\n",
        "\n",
        "Our main goal will be to reconstruct the state space of the postures extracted from Part 1 of the tutorial.\n",
        "\n",
        "But what is a state space, exactly?\n",
        "\n",
        "The state space, or behavior space in our case, is all the possible states a dynamical system can occupy. In a simple one like a pendulum, we would have a simple state space, as this video by Prof. Ghrist Math shows."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Video of the phase-space of a pendulum\n",
        "url = upload_public(directory + 'vid_phase_space_pendulum_Ghrist_Math.mp4')\n",
        "HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ],
      "metadata": {
        "id": "JJZReGPEfU8d"
      },
      "id": "JJZReGPEfU8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A pendulum has a circular state space, with two dimensions: momentum and angular position. At every point in time, the pendulum is located at one point in this circle, and a point in the circle represents all the necessary information to completely describe what the pendulum is doing at this point in time (its location in space and its speed).\n",
        "\n",
        "More complex systems, like our bending larva, will have more complex state spaces, with more dimensions (otherwise called degrees of freedom). The following picture shows an example of a 2D non-linear embedding of C. Elegans' state space (from Antonio Costa, manuscript in progress)."
      ],
      "metadata": {
        "id": "l3N5kLUmfPG1"
      },
      "id": "l3N5kLUmfPG1"
    },
    {
      "cell_type": "code",
      "source": [
        "display_picture(directory + 'img_umap_celegans_costa.png')\n"
      ],
      "metadata": {
        "id": "niQJ9Qyffum5"
      },
      "id": "niQJ9Qyffum5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Different behaviors occupy different parts of the state space and have different shapes. This picture shows that forward crawling has a circular shape, much like the one of our pendulum, which can teach us something about the behavior. We can also see that dorsal turns occupy a smaller subspace than forward crawling. In that sense, a visual representation of the state space can be helpful to analyze behavior qualitatively. It is also possible to extract dynamical quantities from state-space such as Lyapunov exponents or stable periodic orbits, like in Ahamed $\\textit{et al}$ [1], which I encourage you to read if you are interested.\n",
        "\n"
      ],
      "metadata": {
        "id": "n6NVHwwafts4"
      },
      "id": "n6NVHwwafts4"
    },
    {
      "cell_type": "markdown",
      "id": "8cb657cd",
      "metadata": {
        "id": "8cb657cd"
      },
      "source": [
        "## Building a maximally predictive state space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9753f1c",
      "metadata": {
        "id": "c9753f1c"
      },
      "source": [
        "We can now get some intuition about how to reconstruct the state space of our bending larva from the recordings of its posture.\n",
        "\n",
        "In the pendulum case, measuring the angle and velocity is sufficient to recover the complete state space. However, when we study a dynamical system more complicated than a pendulum, we usually cannot know the relevant degrees of freedom to measure. We typically have measurements that are only partial observations of the real dynamics of the system. Indeed, measurements are a (possibly noisy, most likely nonlinear) observation function of the state space that maps onto the real numbers line, so they only provide indirect information about its properties. However, this map is one-to-one, so we know that every time point can be mapped to a single point in state space. Let us watch this video by Sugihara et al., Detecting Causality in Complex Ecosystems, Science, 2012. to illustrate what we mean. In this case, the butterfly-like shape is the state space of the Lorenz system, but we can imagine it to be any state space of any system.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Video to illustrate the concept of state space\n",
        "url = upload_public(directory + 'vid_intro_state_space_Sugihara.mp4')\n",
        "HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ],
      "metadata": {
        "id": "ceNETloUgpEI"
      },
      "id": "ceNETloUgpEI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This video illustrates that from a single measurement (the time series  $X(t)$ for example), we only measure a subset of the variables that define the full state of the system. The unobserved states ($Y(t)$ and $Z(t)$) create a history dependence on $X(t)$. Fortunately, a useful theorem by Takens and others states that given a generic measurement on a dynamical system, it is possible to reconstruct the state space by concatenating enough time delays of the measurement data.\n",
        "\n",
        "Let's come back to our larva. If we knew the number of dimensions the state space that entirely describes the dynamics of larva's bending has, we could measure them all. For now, we only have a 4-dimensional time series of the principal components of our larva's bending behavior. With the Takens theorem, we can look for a map of the bending state space that would conserve its properties. In other words, stacked delayed versions of our measurements will result in a correct embedding of the state space. A second video by Sugihara et al. illustrates this idea.\n",
        "\n"
      ],
      "metadata": {
        "id": "B9-lJpCfgl-K"
      },
      "id": "B9-lJpCfgl-K"
    },
    {
      "cell_type": "code",
      "source": [
        "#Video to illustrate the Takens theorem\n",
        "url = upload_public(directory + 'vid_Takens_thm_Sugihara.mp4')\n",
        "HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ],
      "metadata": {
        "id": "me6PN7gOg7Kz"
      },
      "id": "me6PN7gOg7Kz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Takens theorem guarantees that if we use delayed versions of our measurements as our new axes, we will create a smooth and one-to-one map of the original state space and that it will conserve its topological properties. In the video, the delay is called $\\tau$, but we will call it $K$ in the tutorial. This technique is called time delay embedding."
      ],
      "metadata": {
        "id": "kS2oPQblg7WD"
      },
      "id": "kS2oPQblg7WD"
    },
    {
      "cell_type": "markdown",
      "id": "e16b03f2",
      "metadata": {
        "id": "e16b03f2"
      },
      "source": [
        "\n",
        "In mathematical terms, time delay embedding is the augmentation of a time series $x(t)$ into a higher dimension through the construction of a delay vector:\n",
        "\n",
        "$$\\vec{x}(t) = (x(t), x(t-K),...,x(t-K + 1) $$\n",
        "\n",
        " The determining factors to build the space are the delay $K$ and the embedding dimension $m$.\n",
        "\n",
        "In computational terms, we have a measurement matrix from the last tutorial, which looks like this:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_picture(directory + \"img_measurement_matrix.png\")"
      ],
      "metadata": {
        "id": "XNJDJSsrhOx2"
      },
      "id": "XNJDJSsrhOx2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "Where $d$ is the dimension of the measurement (4 in our case, each measurement being a principal component of the bending data) and $T$ is the length of our time series. We are looking to make a matrix that looks like this:"
      ],
      "metadata": {
        "id": "pK-1OJJChO7w"
      },
      "id": "pK-1OJJChO7w"
    },
    {
      "cell_type": "code",
      "source": [
        "display_picture(directory + \"img_trajectory_matrix.png\")"
      ],
      "metadata": {
        "id": "eztb7-LzhbVX"
      },
      "id": "eztb7-LzhbVX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call this matrix $Y_K$ the trajectory matrix (or delay-embedding matrix). It represents trajectories in the embedded space (or posture sequences), and its size is $K \\times d$ and $ T - K + 1$.\n",
        "\n",
        "The schematics of the matrix are from [1]."
      ],
      "metadata": {
        "id": "Hh3nUUd2hbea"
      },
      "id": "Hh3nUUd2hbea"
    },
    {
      "cell_type": "markdown",
      "id": "5aba0847",
      "metadata": {
        "id": "5aba0847"
      },
      "source": [
        "## Method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12795c36",
      "metadata": {
        "id": "12795c36"
      },
      "source": [
        "The method is divided into three parts.\n",
        "First, we will build the trajectory matrix or posture sequence matrix. For this, we will need to find an appropriate delay $K$. Second, we will calculate an embedding dimension $m$, smaller than the size of the initial matrix, to better visualize and interpret the space. Finally, we will make some quick analyses to visualize the space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0b3078",
      "metadata": {
        "id": "5a0b3078"
      },
      "source": [
        "### Choosing an embedding parameter K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a492963a",
      "metadata": {
        "id": "a492963a"
      },
      "source": [
        "The first step in delay embedding is the choice of a sensible delay for our dataset. We can think of $K$ as the length of a window we are moving through our data. In our trajectory matrix, we will put all sequences of length $K$ against each other, making a $K\\times d$ dimensional space.\n",
        "\n",
        "There are many valid criteria to choose $K$. Ours will be to maximize predictive information, which is why we talk of a maximally predictive posture sequence. The metric we will use is the entropy rate $\\delta_{h_N} (K)$, which is the amount of information that has to be kept in $K-1$ time delays for an accurate forecast of the next step, a metric often used in dynamical systems [2]. We can think about entropy as the amount of \"surprise\" contained in a sequence. A sequence with low entropy has low surprise and good predictive power. The opposite is true of high entropy.\n",
        "\n",
        "$\\textit{Note}$: Since entropy is an information-theoretic quantity based on discrete sequences, we will need to discretize our state space into $N$ partitions, using k-means clustering (see this tutorial for more explanations: https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/ ).\n",
        "\n",
        "We define the entropy rate as\n",
        "\n",
        "$$\\delta_{h_N} (K) = h_N(K - 1) - h_N (K) $$\n",
        "\n",
        "where h_N(K) is the entropy of of the system at $N$ and $K$.\n",
        "\n",
        "For a finite-sized Markovian system (we will explain more in Part 3), there exists a $K^*$ such that $\\delta_{h_N} (K^*) = 0$, which means that the entropy rate does not change as we increase delays. In other words, as we increase the window length ($K$), we do not add information that could increase predictability. If the entropy rate is 0 as we increase $K$, then we removed all history dependence and we have complete information on the system. We are looking for this $K^*$ if it exists. If it does not, we are looking for the $K^*$ at which the entropy rate becomes stable. See [3] for more theoretical details on entropy and its use in this context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd022425",
      "metadata": {
        "id": "cd022425"
      },
      "source": [
        "### Finding a number of partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18273467",
      "metadata": {
        "id": "18273467"
      },
      "source": [
        "As we said, the space must first be topologically discretized with k-means clustering. For each $K$, we find the optimal number of partitions $N^*$. We choose $N^*$ as the biggest possible number of clusters before the entropy starts decreasing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c58ca73f",
      "metadata": {
        "id": "c58ca73f"
      },
      "outputs": [],
      "source": [
        "n_seed_range=np.arange(1,2000,300) #number of partitions to examine. This range can be modified to your liking.\n",
        "n_samples = 3 #number of random partitions made for each number of seeds, in order to have an error measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea95a08",
      "metadata": {
        "id": "dea95a08"
      },
      "outputs": [],
      "source": [
        "#Warning: this will take time !!\n",
        "range_Ks =  np.arange(1,50,5,dtype=int) #range of delays to study, in frames.\n",
        "h_K=[]\n",
        "\n",
        "for K in range_Ks: # for each delay K\n",
        "    print('K =',K)\n",
        "    traj_matrix = embed.trajectory_matrix(PC_ts,K=K-1) # Build a trajectory matrix with K_i\n",
        "    h_seeds = []\n",
        "    for n_partitions in n_seed_range: # For each number of partitions in n_seed_range\n",
        "        h_samples=[]\n",
        "        for idx in range(n_samples): #Repeat n_samples times since partitionning is a random process\n",
        "            labels = cl.kmeans_knn_partition(traj_matrix,n_partitions) #do the paritionning with n_partitions\n",
        "            h = op_calc.get_entropy(labels)  # calculate the entropy associated to that clustering\n",
        "            h_samples.append(h)  #store that number\n",
        "        h_seeds.append(h_samples)\n",
        "        print('N =',n_partitions,\"Entropy = \", h_samples)\n",
        "    h_K.append(h_seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74a5cd3",
      "metadata": {
        "id": "d74a5cd3"
      },
      "outputs": [],
      "source": [
        "#Build a figure of how entropy evolves with the number of partitions for each K\n",
        "\n",
        "colors_K = plt.cm.viridis(np.linspace(0,1,len(range_Ks)))\n",
        "max_h_K=np.zeros(len(range_Ks))\n",
        "cil_h_K=np.zeros(len(range_Ks))\n",
        "ciu_h_K=np.zeros(len(range_Ks))\n",
        "max_idx_array = np.zeros(len(range_Ks), dtype = int)\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "for k in range(len(range_Ks)):\n",
        "    mean = np.mean(h_K[k],axis=1)\n",
        "    cil = np.percentile(h_K[k],2.5,axis=1)\n",
        "    ciu = np.percentile(h_K[k],97.5,axis=1)\n",
        "    plt.errorbar(n_seed_range,mean,c=colors_K[int(k)],yerr = [mean-cil,ciu-mean],capsize=4,marker='o',ms=5)\n",
        "    max_idx = np.argmax(mean)\n",
        "    max_idx_array[k] = int(max_idx)\n",
        "    max_h_K[k]=mean[max_idx]\n",
        "    cil_h_K[k]=cil[max_idx]\n",
        "    ciu_h_K[k]=ciu[max_idx]\n",
        "\n",
        "plt.scatter(n_seed_range,mean,c=mean,vmin=min(range_Ks), vmax=max(range_Ks), s = 0)\n",
        "plt.colorbar(shrink = .7, aspect = 6,label='$K$ (frames)')\n",
        "plt.xlabel('$N$ (partitions)')\n",
        "plt.ylabel('$h$ (nats/symbol)')\n",
        "#plt.xscale('log')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba18421",
      "metadata": {
        "id": "8ba18421"
      },
      "source": [
        "A lot lies in this graph. The x-axis is the number of partitions tested, the y-axis is entropy, and the colors represent each K. We should mention that:\n",
        "\n",
        "1.  At a fixed number of partitions $N$, entropy will decrease as delays increase. Indeed, a larger delay window will contain more information, which increases predictive power.\n",
        "2. For a specific $K$, entropy increases with partitions, reaches a maximum, and then drops.\n",
        "    - If we split the state space into very few states (low number of clusters), predicting which state the system will occupy in the next step becomes trivial, so entropy will be low.\n",
        "    - As we increase the number of clusters, the prediction becomes more challenging, so entropy grows until it reaches a maximum. From there, additional partitions will have an overfitting effect and result in an artificial drop in the entropy. This drop is due to the finite size of the time series. A time series with enough data points would show a plateau instead.\n",
        "\n",
        "Whether you have a plateau or a drop, we take the $N^*$ associated with maximal entropy. In our case, we will take the maximum of each curve to determine the optimal number of partitions for each K.\n",
        "\n",
        "$\\textit{Note:}$ The behavior of $h$ with $N$ indicates that we are in the presence of deterministic chaos rather than a stochastic process. Indeed, the noise characteristic of a stochastic process has information at all scales. The entropy would then increase with $N$ indefinitely. Deterministic chaos has a fractal nature, so there exists a scale at which the entropy stops changing. As we said, the decrease of $h$ here is due to finite-size effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6ca3e1",
      "metadata": {
        "id": "4f6ca3e1"
      },
      "outputs": [],
      "source": [
        "#Print the information about your optimal N for each K\n",
        "print(\"Optimal numbers of partitions for selected delays\")\n",
        "print(\"Delays: \", range_Ks)\n",
        "print(\"Number of partitions\", n_seed_range[max_idx_array])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1f6e20",
      "metadata": {
        "id": "ce1f6e20"
      },
      "source": [
        "### Finding $K^*$  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how many partitions are needed for every value of delays $K$, we can plot the entropy associated to the trajectory matrices built from each those delays."
      ],
      "metadata": {
        "id": "BJpe2XcazcDt"
      },
      "id": "BJpe2XcazcDt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54ce516",
      "metadata": {
        "id": "e54ce516"
      },
      "outputs": [],
      "source": [
        "n_partitions = n_seed_range[max_idx_array]  #Sample the correct number of partitions for each K\n",
        "\n",
        "h_K=[]\n",
        "\n",
        "for K in range(len(range_Ks)):\n",
        "    traj_matrix = embed.trajectory_matrix(PC_ts,K=range_Ks[K]-1) # construct the delay matrix with K\n",
        "    h_samples=[]\n",
        "    for idx in range(n_samples):\n",
        "        labels=cl.kmeans_knn_partition(traj_matrix,n_partitions[K]) #Make the partitioning\n",
        "        h = op_calc.get_entropy(labels)  #Calculate its entropy\n",
        "        h_samples.append(h)  #Store\n",
        "    h_K.append(h_samples)\n",
        "    print(\"K :\", range_Ks[K], \"Entropy : \", h_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1766ca",
      "metadata": {
        "id": "3c1766ca"
      },
      "outputs": [],
      "source": [
        "# Make a figure of the averaged entropy associated with each K.\n",
        "\n",
        "mean = np.mean(h_K,axis=1) # average of each K\n",
        "var = np.std(h_K, axis = 1)  #Standard variation\n",
        "\n",
        "plt.errorbar(range_Ks,mean, yerr = var,marker = \"o\")\n",
        "plt.xlabel('$K$')\n",
        "plt.ylabel('$h$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e284bdae",
      "metadata": {
        "id": "e284bdae"
      },
      "source": [
        "We note that as expected, the entropy decreases with increasing $K$, then seems to stabilize. Remember that we are looking for the $K^*$ at which the entropy stabilizes. We calculate the derivative of the entropy ($\\Delta K$), and choose the K for which it reaches $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf59faa",
      "metadata": {
        "id": "ccf59faa"
      },
      "outputs": [],
      "source": [
        "# Take the derivative of average K. We don't explicitely divide by the time step as it is the\n",
        "# same for all Ks\n",
        "delh = np.empty(range_Ks.shape[0])\n",
        "delh[:] = np.nan\n",
        "delh[1:] = mean[1:]-mean[:-1]\n",
        "\n",
        "# To get an error bar, we take the variance for all the samples for each K\n",
        "h_diff = np.zeros((range_Ks.shape[0], n_samples))\n",
        "h_K = np.array(h_K)\n",
        "h_diff[1:,:] = h_K[1:,:] - h_K[:-1,:]\n",
        "var_diff = np.std(h_diff, axis = 1)  #Calculate the variance\n",
        "\n",
        "plt.errorbar(range_Ks,delh,yerr = var_diff)\n",
        "plt.xlabel('$K$')\n",
        "plt.ylabel('$h_k-h_{K-1}$')\n",
        "plt.hlines(0.,range_Ks[0],range_Ks[-1],colors='k',linestyles='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4dcc16",
      "metadata": {
        "id": "9e4dcc16"
      },
      "source": [
        "The entropy stops varying around $K^* = 25$ frames. It becomes our official value for the optimal delay to maximize predictability in our dataset. The time conversion for $K^*$ is about 4 seconds for a frame rate of 5 seconds.\n",
        "\n",
        "This means there are some relevant dynamics occurring in the 4-second range in our dataset. It is a good idea to take a step back and wonder: what kind of dynamics occur in this time frame?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9d9efe",
      "metadata": {
        "id": "1d9d9efe"
      },
      "source": [
        "### Dimensionality reduction ($m$)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe47dad9",
      "metadata": {
        "id": "fe47dad9"
      },
      "source": [
        "We now have a trajectory matrix of size $K \\times d$ and $ T - K + 1$. A space of this size is hard to visualize, and all dimensions might not be equally impactful. We use a dimensionality reduction technique called singular value decomposition to reduce the space to its most meaningful components. Singular value decomposition follows the following idea.\n",
        "\n",
        "If we have a matrix\n",
        "\n",
        "$$\n",
        "X = \\begin{pmatrix}\\vec{x}_1^T & \\vec{x}_2 ^T & ... & \\vec{x}_m ^T  \\\\ \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "we can prove it can be decomposed into three matrices\n",
        "\n",
        "$$\n",
        "U\\Sigma V^T = \\begin{pmatrix}\\vec{u}_1^T & \\vec{u}_2^T & ... & \\vec{u}_m^T  \\\\ \\end{pmatrix}\\begin{pmatrix}\\sigma_1& 0& ... & 0  \\\\ 0 & \\sigma_2&  ... & 0  \\\\ 0 & 0&  ... & 0\\\\ 0 & 0& ... & \\sigma_m \\\\ \\end{pmatrix}\\begin{pmatrix}\\vec{v}_1^T & \\vec{v}_2^T & ... & \\vec{v}_m^T  \\\\ \\end{pmatrix}^T\n",
        "$$\n",
        "\n",
        "where the columns of $U$ are the eigenvectors of $X$ and $VV^T = I$, and $\\Sigma$ is a diagonal matrix. I suggest reading [4] for more detailed information.\n",
        "\n",
        "\n",
        "In our context, $X$ is the trajectory matrix, with each column being a  delayed time series of length $ T - K + 1$. Think of each column as a dimension axis for our state space. The $U$ matrix contains all the $K*D$ singular vectors of length $ T - K + 1$. These modes are another basis for the space described by $X$. $\\Sigma$ is a diagonal matrix with the variance associated with each mode on its diagonal. The columns of the last matrix, $V^T$, represent the proportion of each singular mode necessary to reconstruct the data. The rows indicate how the proportions of a singular mode evolve across all the dimensions.  \n",
        "\n",
        "\n",
        "\n",
        "We will do an SVD on our data. Like with $K*$, we will find the smallest number of singular modes $m*$ needed to minimize entropy while keeping the entropy rate stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a28263",
      "metadata": {
        "id": "51a28263"
      },
      "outputs": [],
      "source": [
        "K_opt = 25  # to enter manually from the entropy rate plot.\n",
        "traj_matrix = embed.trajectory_matrix(PC_ts,K=K_opt-1)  #Make a trajectory matrix with optimal K.\n",
        "u, s, v = scipy.linalg.svd(traj_matrix, full_matrices=0)  # Make a singular value decomposition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1280d3ef",
      "metadata": {
        "id": "1280d3ef"
      },
      "source": [
        "Like with $K$, we will find the maximally predictive number of partitions for each number of dimensions $m$. To do so, we calculate the trajectory matrix with the optimal $K$, then iteratively take an increasing number of dimensions from the singular value decomposition, and calculate how the predictivity is modulated. We will take the smallest number of dimensions for which the entropy is stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad687936",
      "metadata": {
        "id": "ad687936"
      },
      "outputs": [],
      "source": [
        "n_seed_range=np.arange(100,2000,200) #number of partitions to examine\n",
        "n_seed_range = np.insert(n_seed_range,0,10)\n",
        "n_samples = 3 #number of random partitions made for each number of seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "932dc26c",
      "metadata": {
        "id": "932dc26c"
      },
      "outputs": [],
      "source": [
        "range_Ms =  np.arange(1,8,1,dtype=int) #range of dimensions to incorporate\n",
        "h_M=[]\n",
        "\n",
        "for M in range_Ms:\n",
        "    print('M =',M)\n",
        "    traj_matrix = np.dot(u, np.diag(s))[:,:M]  #We ponderate each singular vector by it's singular value and keep M vectors\n",
        "    h_seeds=[]\n",
        "    for n_seeds in n_seed_range:\n",
        "        h_samples=[]\n",
        "        for idx in range(n_samples):\n",
        "            labels=cl.kmeans_knn_partition(traj_matrix,n_seeds)  #We make the partitioning\n",
        "            h = op_calc.get_entropy(labels)  #Calculate the entropy\n",
        "            h_samples.append(h)\n",
        "        h_seeds.append(h_samples)\n",
        "        print('N =',n_seeds,\"Entropy = \", h_samples)\n",
        "    h_M.append(h_seeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e570850f",
      "metadata": {
        "id": "e570850f"
      },
      "outputs": [],
      "source": [
        "colors_M = plt.cm.viridis(np.linspace(0,1,len(range_Ms)))\n",
        "max_h_M=np.zeros(len(range_Ms))\n",
        "cil_h_M=np.zeros(len(range_Ms))\n",
        "ciu_h_M=np.zeros(len(range_Ms))\n",
        "max_idx_array = np.zeros(len(range_Ms), dtype = int)\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "for m in range(len(range_Ms)):\n",
        "    mean = np.mean(h_M[m],axis=1)\n",
        "    cil = np.percentile(h_M[m],2.5,axis=1)\n",
        "    ciu = np.percentile(h_M[m],97.5,axis=1)\n",
        "    plt.errorbar(n_seed_range,mean,c=colors_M[int(m)],yerr = [mean-cil,ciu-mean],capsize=4,marker='o',ms=5)\n",
        "    max_idx = np.argmax(mean)\n",
        "    max_idx_array[m] = int(max_idx)\n",
        "    max_h_M[m]=mean[max_idx]\n",
        "    cil_h_M[m]=cil[max_idx]\n",
        "    ciu_h_M[m]=ciu[max_idx]\n",
        "plt.scatter(n_seed_range,mean,c=mean,vmin=min(range_Ms), vmax=max(range_Ms), s = 0)\n",
        "plt.colorbar(shrink = .7, aspect = 6,label='$M$')\n",
        "plt.xlabel('$N$ (partitions)')\n",
        "plt.ylabel('$h$ (nats/symbol)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00a2bcfa",
      "metadata": {
        "id": "00a2bcfa"
      },
      "source": [
        "The curves for the number of dimensions look a lot like the ones for $K$. It makes sense that an increasing number of dimensions should increase the amount of information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "830b7d81",
      "metadata": {
        "id": "830b7d81"
      },
      "outputs": [],
      "source": [
        "print(\"Optimal numbers of partitions for selected delays\")\n",
        "print(\"Dimensions: \", range_Ks)\n",
        "print(\"Number of partitions\", n_seed_range[max_idx_array])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf8f6ea",
      "metadata": {
        "id": "cdf8f6ea"
      },
      "outputs": [],
      "source": [
        "n_seeds = n_seed_range[max_idx_array] # We take the optimal partitioning for each M\n",
        "\n",
        "h_M=[]\n",
        "\n",
        "for M in range(len(range_Ms)):\n",
        "    traj_matrix = np.dot(u, np.diag(s))[:,:range_Ms[M]]  #Construct the traj matrix with M dimensions\n",
        "    h_samples=[]\n",
        "    for idx in range(n_samples):\n",
        "        labels=cl.kmeans_knn_partition(traj_matrix,n_seeds[M])  #Make the partitioning\n",
        "        h = op_calc.get_entropy(labels)  #Calculate the entropy\n",
        "        h_samples.append(h)\n",
        "    h_M.append(h_samples)\n",
        "    print(\"M: \", range_Ms[M],\"Entropy: \", h_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b6d91c",
      "metadata": {
        "id": "68b6d91c"
      },
      "outputs": [],
      "source": [
        "mean = np.mean(h_M,axis=1)\n",
        "var = np.std(h_M, axis = 1)\n",
        "plt.errorbar(range_Ms,mean, yerr = var, marker = \"o\")\n",
        "plt.xlabel('$M$')\n",
        "plt.ylabel('$h$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8317ea78",
      "metadata": {
        "id": "8317ea78"
      },
      "outputs": [],
      "source": [
        "delh = np.empty(range_Ms.shape[0])\n",
        "delh[:] = np.nan\n",
        "delh[1:] = mean[1:]-mean[:-1]\n",
        "h_diff = np.zeros((range_Ms.shape[0], n_samples))\n",
        "h_M = np.array(h_M)\n",
        "h_diff[1:,:] = h_M[1:,:] - h_M[:-1,:]\n",
        "var_diff = np.std(h_diff, axis = 1)\n",
        "\n",
        "plt.errorbar(range_Ms,delh,yerr = var_diff, marker = \"o\")\n",
        "plt.xlabel('$M$')\n",
        "plt.ylabel('$h_m-h_{m-1}$')\n",
        "plt.hlines(0.,range_Ms[0],range_Ms[-1],colors='k',linestyles='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583d81e2",
      "metadata": {
        "id": "583d81e2"
      },
      "source": [
        "We found our optimal number of dimensions: $m^* = 5$, after which the entropy rate becomes stable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ef8bc2",
      "metadata": {
        "id": "a3ef8bc2"
      },
      "source": [
        "# Some analysis on the state-space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6247f850",
      "metadata": {
        "id": "6247f850"
      },
      "source": [
        "We now reconstructed the state-space with optimal parameters $K^*$ frames and $m*$ dimensions. Here, we give some examples of analyses to get more familiar with the space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b386d5",
      "metadata": {
        "id": "02b386d5"
      },
      "outputs": [],
      "source": [
        "#Build the space with optimal parameters\n",
        "K_opt = 25\n",
        "M_opt = 5\n",
        "traj_matrix = embed.trajectory_matrix(PC_ts,K=K_opt - 1)\n",
        "u, s, v = scipy.linalg.svd(traj_matrix, full_matrices=0)\n",
        "\n",
        "#We must not forget to ponderate u with s, otherwise the singular vectors have proportions that do not reflect the\n",
        "# dataset\n",
        "u_var = np.dot(u, np.diag(s))[:,:M_opt]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c02c4bd",
      "metadata": {
        "id": "3c02c4bd"
      },
      "source": [
        "Let us start by looking how the singular values are distributed in general"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423f4403",
      "metadata": {
        "id": "423f4403"
      },
      "outputs": [],
      "source": [
        "plt.plot(s, \"-o\")\n",
        "#plt.semilogy(s, \"-o\")\n",
        "plt.xlabel(\"Singular mode\")\n",
        "plt.ylabel(\"Singular value\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e8f657",
      "metadata": {
        "id": "80e8f657"
      },
      "source": [
        "It is interesting to see that the first five singular modes are dominant, which confirms our choice of 5 dimensions to maximize predictability. We also note that the first mode is vastly dominant over others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06619539",
      "metadata": {
        "id": "06619539"
      },
      "source": [
        "We might want to have an overall view of our state-space. For that, we will use 2D non-linear embedding called UMAP. We will use a color bar to illustrate time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b26353",
      "metadata": {
        "id": "c1b26353"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(n_neighbors=200,min_dist=0.1)\n",
        "embedding = reducer.fit_transform(u_var)\n",
        "plt.scatter(embedding[:,0], embedding[:, 1],c= np.linspace(0,1,len(u_var)), s = 0.8)\n",
        "plt.colorbar()\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187a2024",
      "metadata": {
        "id": "187a2024"
      },
      "source": [
        "This shape is very interesting and structured. A deeper analysis might help us figure out what each of these sections corresponds to in the animal behavior. We will get deeper into it in the next tutorial.\n",
        "\n",
        "We can check if the dimensionality reduction is sensible by looking at how the space changes if we plot the full trajectory matrix. Both plots should look alike, as they describe the same space. Let's check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f39dc5",
      "metadata": {
        "id": "f1f39dc5"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(n_neighbors=100,min_dist=0.1)\n",
        "embedding = reducer.fit_transform(traj_matrix)\n",
        "plt.scatter(embedding[:,0], embedding[:, 1],c= np.linspace(0,1,len(traj_matrix)), s = 0.8)\n",
        "plt.colorbar()\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921809b2",
      "metadata": {
        "id": "921809b2"
      },
      "source": [
        "As expected, they have very similar structures. To validate the importance to weight our $U$ matrix vectors with their respective variance, let's look at the UMAP of U."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177ffffb",
      "metadata": {
        "id": "177ffffb"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(n_neighbors=200,min_dist=0.1)\n",
        "embedding = reducer.fit_transform(u)\n",
        "plt.scatter(embedding[:,0], embedding[:, 1],c= np.linspace(0,1,len(u)), s = 0.8)\n",
        "plt.colorbar()\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed5480b",
      "metadata": {
        "id": "9ed5480b"
      },
      "source": [
        "You see that in this case, we loose all structure. Finally, let's look at the space without embedding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "581ec4c6",
      "metadata": {
        "id": "581ec4c6"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(n_neighbors=200,min_dist=0.1)\n",
        "embedding = reducer.fit_transform(PC_ts)\n",
        "plt.scatter(embedding[:,0], embedding[:, 1],c= np.linspace(0,1,len(PC_ts)), s = 0.8)\n",
        "plt.colorbar()\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8814e5ba",
      "metadata": {
        "id": "8814e5ba"
      },
      "source": [
        "There is some structure, but less than in the embedded space. By choosing $K^*$ and $m^*$ to minimize the entropy, we built a maximally predictive sequence of postures to recover this additional information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8b68b2",
      "metadata": {
        "id": "9a8b68b2"
      },
      "source": [
        "We might want to make sense of the structures in this embedded space. For further analysis, it is good practice to study the SVD modes. To interpret them, you could relate them to meaningful parameters, look at their time series, etc. It might or might not be useful, depending on your data. For the larva, the first SVD modes are not easily interpretable. However, we could show that the first SVD mode has a very high direct correlation with the anterior angle of the larva. However, if we did a similar process looking at the compression of the segments (peristalsis of the larva), we would get interpretable more SVD modes (all account for the wave going through the larva's body).\n",
        "\n",
        "\n",
        "As for your own data, we leave it to your creativity and specific needs!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "407bcb89",
      "metadata": {
        "id": "407bcb89"
      },
      "source": [
        "\n",
        "Note that with your own data, you might need to tweak the parameters of UMAP embedding, with are n_neighbors and min_dist."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ed29fc",
      "metadata": {
        "id": "16ed29fc"
      },
      "source": [
        "References\n",
        "\n",
        "[1] Ahamed, T. et al., Capturing the continuous complexity of behaviour in Caenorhabditis elegans. Nat. Phys., 2019.\n",
        "\n",
        "[2] Grassberger, P., Toward a quantitative theory of self-generated complexity, International Journal of Theoretical Physics volume, 1986.\n",
        "\n",
        "[3] Costa. C. A, et al., Maximally predictive states: From partial observations to long timescales, Chaos, 2023.\n",
        "\n",
        "[4] J. Shlens, A tutorial on principal component analysis, arXiv, 2014."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}